{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwqG4eQiHDHY"
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch PyPDF2 pdfplumber datasets scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moqwHKDUDN2N"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVNkctNeH1Py"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdvVMu9nG-fr"
   },
   "outputs": [],
   "source": [
    "# extract text and load documents\n",
    "def extract_text_from_pdf_pypdf2(file_bytes, filename=\"<uploaded_file>\"):\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))\n",
    "        text = []\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text.append(page_text)\n",
    "        print(f\"Successfully extracted text from '{filename}' using PyPDF2.\")\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from '{filename}' using PyPDF2: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf_pdfplumber(file_bytes, filename=\"<uploaded_file>\"):\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
    "            text = []\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text.append(page_text)\n",
    "        print(f\"Successfully extracted text from '{filename}' using pdfplumber.\")\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from '{filename}' using pdfplumber: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf_path(pdf_path):\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File not found at path: '{pdf_path}'. Ensure it's uploaded to session storage.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            file_bytes = f.read()\n",
    "\n",
    "        extracted_text = extract_text_from_pdf_pdfplumber(file_bytes, filename)\n",
    "        if not extracted_text:\n",
    "            extracted_text = extract_text_from_pdf_pypdf2(file_bytes, filename)\n",
    "\n",
    "        return extracted_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing or reading PDF from path '{pdf_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_documents_for_qa():\n",
    "    all_documents_text = {} # To store text from all sources {identifier: text}\n",
    "    print(\"1. Load PDF files by path from Colab session storage\")\n",
    "    print(\"2. Provide text directly\")\n",
    "    print(\"3. Do both (load PDFs by path and provide text directly)\")\n",
    "\n",
    "    choice = input(\"Enter your choice (1, 2, or 3): \").strip()\n",
    "\n",
    "    if choice in ['1', '3']:\n",
    "        print(\"Enter PDF file paths (one per line, leave empty and press Enter twice when done):\")\n",
    "\n",
    "        pdf_paths = []\n",
    "        while True:\n",
    "            line = input().strip()\n",
    "            if not line:\n",
    "                break\n",
    "            pdf_paths.append(line)\n",
    "\n",
    "        if pdf_paths:\n",
    "            print(\"\\nProcessing PDF(s) from provided paths...\")\n",
    "            for pdf_path in pdf_paths:\n",
    "                if not pdf_path: continue # Skip empty lines\n",
    "                filename = os.path.basename(pdf_path)\n",
    "                print(f\"  Processing '{filename}' from path: '{pdf_path}'\")\n",
    "                extracted_text = extract_text_from_pdf_path(pdf_path)\n",
    "                if extracted_text:\n",
    "                    # Use the full path as the identifier to avoid conflicts if filenames are same\n",
    "                    all_documents_text[pdf_path] = extracted_text\n",
    "                    print(f\"  Successfully processed '{filename}'.\")\n",
    "                else:\n",
    "                    print(f\"  Warning: Could not process '{filename}' from path. Skipping.\")\n",
    "        else:\n",
    "            print(\"No PDF paths were provided.\")\n",
    "\n",
    "    if choice in ['2', '3']:\n",
    "        print(\"Please paste your text here (press Enter twice to finish):\")\n",
    "        user_direct_text_lines = []\n",
    "        while True:\n",
    "            line = input()\n",
    "            if not line:\n",
    "                break\n",
    "            user_direct_text_lines.append(line)\n",
    "\n",
    "        full_direct_text = \"\\n\".join(user_direct_text_lines).strip()\n",
    "\n",
    "        if full_direct_text:\n",
    "            direct_input_id = f\"direct_text_input_{len(all_documents_text) + 1}\"\n",
    "            all_documents_text[direct_input_id] = full_direct_text\n",
    "            print(f\"Direct text input received and stored as '{direct_input_id}'.\")\n",
    "        else:\n",
    "            print(\"No direct text was provided.\")\n",
    "\n",
    "    if not all_documents_text:\n",
    "        print(\"\\nNo documents (PDFs from paths or direct text) were loaded. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n--- All Documents Loaded ---\")\n",
    "    for identifier, text_content in all_documents_text.items():\n",
    "        print(f\"\\n--- Content from: {identifier} ({len(text_content)} characters) ---\")\n",
    "        print(text_content[:200] + \"...\" if len(text_content) > 500 else text_content) # preview\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "    return all_documents_text\n",
    "\n",
    "\n",
    "loaded_documents = load_documents_for_qa() # all_documents_text\n",
    "all_documents_text=loaded_documents\n",
    "\n",
    "if loaded_documents:\n",
    "  print(f\"\\nSuccessfully loaded {len(loaded_documents)} document(s) for QA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text_for_vectorization(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()] # Remove non-alphabetic\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Global variable to store loaded word embedding model\n",
    "word_embedding_model_cache = {}\n",
    "\n",
    "def load_word_embedding_model(model_name=\"glove-wiki-gigaword-50\"):\n",
    "    if model_name not in word_embedding_model_cache:\n",
    "        print(f\"\\n--- Loading pre-trained word embedding model: {model_name} ---\")\n",
    "        try:\n",
    "            model = api.load(model_name)\n",
    "            word_embedding_model_cache[model_name] = model\n",
    "            print(f\"Model '{model_name}' loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading word embedding model '{model_name}': {e}\")\n",
    "            word_embedding_model_cache[model_name] = None\n",
    "    return word_embedding_model_cache[model_name]\n",
    "\n",
    "def get_averaged_word_vector(text, model):\n",
    "    \"\"\"\n",
    "    Calculates the average vector for a piece of text using word embeddings.\n",
    "    Handles words not in vocabulary by skipping them.\n",
    "    Returns a 1D numpy array.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "\n",
    "    tokens = word_tokenize(preprocess_text_for_vectorization(text))\n",
    "\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.key_to_index:\n",
    "            vectors.append(model[token])\n",
    "\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector of the correct dimension\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def get_answer_classical_nlp(question, documents_dict, method=\"tfidf\", word_embedding_model_name=\"glove-wiki-gigaword-50\"):\n",
    "    if not documents_dict:\n",
    "        return {'answer': \"No documents provided.\", 'confidence': 0.0, 'source_document': None, 'source_chunk': None}\n",
    "    best_answer = {'answer': \"No direct answer found in documents.\", 'confidence': 0.0, 'source_document': None, 'source_chunk': None}\n",
    "\n",
    "    # Iterate through each document to find the best sentence/chunk\n",
    "    for doc_id, full_text_content in documents_dict.items():\n",
    "        if not full_text_content.strip():\n",
    "            continue # Skip empty documents\n",
    "\n",
    "        sentences = sent_tokenize(full_text_content) # Split document into sentences\n",
    "        if not sentences:\n",
    "            continue\n",
    "\n",
    "        # Prepare corpus for the chosen vectorizer\n",
    "        processed_sentences = [preprocess_text_for_vectorization(s) for s in sentences]\n",
    "        question_processed = preprocess_text_for_vectorization(question)\n",
    "\n",
    "        if method == \"tfidf\":\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            all_vectors = vectorizer.fit_transform(processed_sentences + [question_processed])\n",
    "            sentence_vectors = all_vectors[:-1]\n",
    "            q_vector = all_vectors[-1]\n",
    "        elif method == \"bow\":\n",
    "            vectorizer = CountVectorizer()\n",
    "            all_vectors = vectorizer.fit_transform(processed_sentences + [question_processed])\n",
    "            sentence_vectors = all_vectors[:-1]\n",
    "            q_vector = all_vectors[-1]\n",
    "        elif method == \"word_embedding\":\n",
    "            model = load_word_embedding_model(word_embedding_model_name)\n",
    "            if model is None:\n",
    "                print(f\"Skipping {word_embedding_model_name} due to model loading error.\")\n",
    "                continue\n",
    "\n",
    "            q_vector = get_averaged_word_vector(question_processed, model)\n",
    "            if np.all(q_vector == 0): # Check if question vector is all zeros\n",
    "                print(f\"  Warning: Question vector is all zeros for '{question}'. Cannot compute similarity.\")\n",
    "                continue\n",
    "\n",
    "            sentence_vectors = np.array([get_averaged_word_vector(s, model) for s in processed_sentences])\n",
    "            # Filter out None or all-zero vectors from sentences\n",
    "            valid_indices = [i for i, vec in enumerate(sentence_vectors) if vec is not None and not np.all(vec == 0)]\n",
    "            if not valid_indices:\n",
    "                continue\n",
    "            sentence_vectors = sentence_vectors[valid_indices]\n",
    "            sentences_for_scoring = [sentences[i] for i in valid_indices] # Corresponding original sentences\n",
    "        else:\n",
    "            return best_answer\n",
    "\n",
    "        if q_vector is None or (method != \"word_embedding\" and q_vector.nnz == 0) or (method == \"word_embedding\" and np.all(q_vector == 0)):\n",
    "             print(f\"  Warning: Question vector is empty/zero for '{question}'. Cannot compute similarity for {doc_id}.\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        # Calculate similarities and find the best sentence within this document\n",
    "        local_best_score = 0.0\n",
    "        local_best_sentence = None\n",
    "\n",
    "        if q_vector.ndim == 1:\n",
    "            q_vector = q_vector.reshape(1, -1)\n",
    "\n",
    "        if hasattr(sentence_vectors, 'ndim') and sentence_vectors.ndim == 1:\n",
    "            sentence_vectors = sentence_vectors.reshape(1, -1)\n",
    "\n",
    "\n",
    "        if isinstance(sentence_vectors, (np.ndarray, list)): # For dense arrays (word embeddings)\n",
    "            if sentence_vectors.ndim == 1: # Handle single sentence case\n",
    "                sentence_vectors = np.array(sentence_vectors).reshape(1, -1)\n",
    "            similarities_array = cosine_similarity(q_vector, sentence_vectors).flatten()\n",
    "        else: # For sparse matrices (TFIDF/BoW)\n",
    "            similarities_array = cosine_similarity(q_vector, sentence_vectors).flatten()\n",
    "\n",
    "        for i, score in enumerate(similarities_array):\n",
    "            current_sentence = sentences_for_scoring[i] if method == \"word_embedding\" else sentences[i]\n",
    "            if score > local_best_score:\n",
    "                local_best_score = score\n",
    "                local_best_sentence = current_sentence\n",
    "\n",
    "        if local_best_score > best_answer['confidence']:\n",
    "            best_answer['answer'] = local_best_sentence if local_best_sentence else \"No relevant sentence found.\"\n",
    "            best_answer['confidence'] = local_best_score\n",
    "            best_answer['source_document'] = doc_id\n",
    "            best_answer['source_chunk'] = local_best_sentence\n",
    "\n",
    "    return best_answer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_question_1 = \"What is BERT?\"\n",
    "    print(\"\\n--- TF-IDF Based QA ---\")\n",
    "    answer_data = get_answer_classical_nlp(test_question_1, all_documents_text, method=\"tfidf\")\n",
    "    print(f\"  Answer: '{answer_data['answer']}'\")\n",
    "    print(f\"  Confidence: {answer_data['confidence']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\n--- Bag of Words (BoW) Based QA ---\")\n",
    "    answer_data = get_answer_classical_nlp(test_question_1, all_documents_text, method=\"bow\")\n",
    "    print(f\"  Answer: '{answer_data['answer']}'\")\n",
    "    print(f\"  Confidence: {answer_data['confidence']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\n--- Word Embedding (GloVe 50) Based QA ---\")\n",
    "    # api.load(\"glove-wiki-gigaword-50\", return_path=True) # To ensure it's downloaded\n",
    "    answer_data = get_answer_classical_nlp(test_question_1, all_documents_text, method=\"word_embedding\", word_embedding_model_name=\"glove-wiki-gigaword-50\")\n",
    "    print(f\"  Answer: '{answer_data['answer']}'\")\n",
    "    print(f\"  Confidence: {answer_data['confidence']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\n--- Word Embedding (Word2Vec Google News 300) Based QA ---\")\n",
    "    answer_data = get_answer_classical_nlp(test_question_1, all_documents_text, method=\"word_embedding\", word_embedding_model_name=\"word2vec-google-news-300\")\n",
    "    print(f\"  Answer: '{answer_data['answer']}'\")\n",
    "    print(f\"  Confidence: {answer_data['confidence']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgQ6KtoWIKPo"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using a pipeline without specifying a model name and revision.*\")\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading tokenizer for '{model_name}'...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loading model '{model_name}' for question answering...\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    # qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "    print(\"QA Pipeline created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    qa_pipeline = None\n",
    "\n",
    "\n",
    "def get_answer_from_context(question, context):\n",
    "    if not qa_pipeline:\n",
    "        return \"Error: QA Pipeline not initialized.\", 0.0\n",
    "    if not context or not context.strip():\n",
    "        return \"Error: Context is empty.\", 0.0\n",
    "    if not question or not question.strip():\n",
    "        return \"Error: Question is empty.\", 0.0\n",
    "\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        return result['answer'], result['score']\n",
    "    except Exception as e:\n",
    "        print(f\"Error during question answering: {e}\")\n",
    "        if len(tokenizer.encode(question, context)) > tokenizer.model_max_length:\n",
    "             return f\"Error: Combined length of question and context ({len(tokenizer.encode(question, context))} tokens) exceeds model's maximum ({tokenizer.model_max_length} tokens). Consider chunking the context.\", 0.0\n",
    "        return \"Error processing question.\", 0.0\n",
    "\n",
    "\n",
    "# Chunking and Reranking\n",
    "if not qa_pipeline:\n",
    "    print(\"QA pipeline not initialized.\")\n",
    "else:\n",
    "    def chunk_text(text, tokenizer, max_chunk_size=384, overlap_size=128):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        effective_max_chunk_size = max_chunk_size\n",
    "\n",
    "        for i in range(0, len(tokens), effective_max_chunk_size - overlap_size):\n",
    "            chunk_tokens = tokens[i : i + effective_max_chunk_size]\n",
    "            chunks.append(tokenizer.decode(chunk_tokens))\n",
    "        return chunks\n",
    "\n",
    "    def get_best_answer(question, documents_dict, confidence_threshold=0.1):\n",
    "        all_possible_answers = []\n",
    "\n",
    "        if not question or not question.strip():\n",
    "            print(\"Error: Question is empty.\")\n",
    "            return all_possible_answers\n",
    "\n",
    "        if not documents_dict:\n",
    "            print(\"No documents provided to search.\")\n",
    "            return all_possible_answers\n",
    "\n",
    "\n",
    "        for filename, full_text_content in documents_dict.items():\n",
    "            print(f\"  Processing document: {filename}\")\n",
    "            if not full_text_content.strip():\n",
    "                print(f\"    Skipping empty document: {filename}\")\n",
    "                continue\n",
    "\n",
    "            if len(tokenizer.encode(question, full_text_content)) < tokenizer.model_max_length:\n",
    "                answer, score = get_answer_from_context(question, full_text_content)\n",
    "                if answer and answer != \"Error: Context is empty.\" and answer != \"Error processing question.\":\n",
    "                    all_possible_answers.append({\n",
    "                        \"answer\": answer,\n",
    "                        \"score\": score,\n",
    "                        \"source_document\": filename,\n",
    "                        \"source_chunk\": full_text_content, # The whole document is the chunk here\n",
    "                        \"context_preview\": full_text_content[:150] + \"...\" if len(full_text_content) > 150 else full_text_content\n",
    "                    })\n",
    "            else:\n",
    "                chunks = chunk_text(full_text_content, tokenizer, max_chunk_size=384, overlap_size=128)\n",
    "                print(f\"    Split into {len(chunks)} chunks.\")\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if not chunk.strip():\n",
    "                        continue\n",
    "                    # A advanced step could involve filtering chunks by semantic similarity to the question to reduce unnecessary QA calls.\n",
    "                    answer, score = get_answer_from_context(question, chunk)\n",
    "                    if answer and answer != \"Error: Context is empty.\" and answer != \"Error processing question.\":\n",
    "                        all_possible_answers.append({\n",
    "                            \"answer\": answer,\n",
    "                            \"score\": score,\n",
    "                            \"source_document\": filename,\n",
    "                            \"source_chunk\": chunk,\n",
    "                            \"context_preview\": chunk[:150] + \"...\" if len(chunk) > 150 else chunk\n",
    "                        })\n",
    "\n",
    "        ranked_answers = sorted(all_possible_answers, key=lambda x: x['score'], reverse=True)\n",
    "        best_answer = ranked_answers[0]\n",
    "        if best_answer['score'] >= confidence_threshold:\n",
    "            return best_answer\n",
    "        else:\n",
    "            print(f\"Top answer found, but its confidence ({best_answer['score']:.4f}) is below the threshold ({confidence_threshold}).\")\n",
    "            return None\n",
    "\n",
    "    question = \"What are embeddings?\"\n",
    "    best_answer = get_best_answer(question, all_documents_text , confidence_threshold=0.2)\n",
    "    \n",
    "    print(f\"\\n--- Final Best Answer for Question: '{question}' ---\")\n",
    "    if best_answer:\n",
    "        print(f\"  Answer: {best_answer['answer']}\")\n",
    "        print(f\"  Confidence Score: {best_answer['score']:.4f}\")\n",
    "        print(f\"  Source Document: {best_answer['source_document']}\")\n",
    "        print(f\"  Context Preview: {best_answer['context_preview']}\")\n",
    "        print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"  No answer found with sufficient confidence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sOzXAvyShsg"
   },
   "outputs": [],
   "source": [
    "# Fine-Tuning BERT\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForQuestionAnswering,\n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        EarlyStoppingCallback\n",
    "    )\n",
    "    from datasets import Dataset, Features, Value\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import json\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    json_file_path = \"dummy_squad_data.json\" # Name of your JSON file\n",
    "\n",
    "    if not os.path.exists(json_file_path):\n",
    "        raise FileNotFoundError(f\"The file '{json_file_path}' was not found.\")\n",
    "\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "        # Assuming the JSON structure has a \"data\" key that contains the list of QA pairs\n",
    "        dummy_squad_data = json_data.get(\"data\", [])\n",
    "        if not dummy_squad_data:\n",
    "            raise ValueError(\"JSON file does not contain a 'data' key with the SQuAD formatted list.\")\n",
    "\n",
    "    print(f\"Loaded {len(dummy_squad_data)} entries from '{json_file_path}'.\")\n",
    "\n",
    "    features = Features({\n",
    "        'id': Value('string'),\n",
    "        'title': Value('string'),\n",
    "        'context': Value('string'),\n",
    "        'question': Value('string'),\n",
    "        'answers': Features({\n",
    "            'answer_start': [Value('int32')],\n",
    "            'text': [Value('string')]\n",
    "        })\n",
    "    })\n",
    "\n",
    "    full_dataset = Dataset.from_list(dummy_squad_data, features=features)\n",
    "    print(\"Full dummy dataset created successfully:\")\n",
    "    print(full_dataset)\n",
    "\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    # Randomly sample indices for train and validation\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=val_size, random_state=42)\n",
    "\n",
    "    train_dataset = full_dataset.select(train_indices)\n",
    "    eval_dataset = full_dataset.select(val_indices)\n",
    "\n",
    "    base_model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(base_model_name).to(device)\n",
    "    print(\"Model and tokenizer loaded.\")\n",
    "\n",
    "    print(\"\\n3. Tokenizing the custom dataset...\")\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        questions = [q.strip() for q in examples[\"question\"]]\n",
    "        contexts = examples[\"context\"]\n",
    "        answers = examples[\"answers\"]\n",
    "\n",
    "        tokenized_examples = tokenizer(\n",
    "            questions,\n",
    "            contexts,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offsets in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            answer = answers[i]\n",
    "            if len(answer['answer_start']) == 0:\n",
    "                start_char = 0\n",
    "                end_char = 0\n",
    "            else:\n",
    "                start_char = answer[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "            context_start_token = sequence_ids.index(1)\n",
    "            context_end_token = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "            if not (offsets[context_start_token][0] <= start_char and offsets[context_end_token][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                token_start_index = context_start_token\n",
    "                while token_start_index <= context_end_token and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "\n",
    "                token_end_index = context_end_token\n",
    "                while token_end_index >= context_start_token and offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "\n",
    "        tokenized_examples[\"start_positions\"] = start_positions\n",
    "        tokenized_examples[\"end_positions\"] = end_positions\n",
    "        return tokenized_examples\n",
    "\n",
    "    # Apply the preprocessing function to the training and validation datasets\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "    tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "    print(\"Dataset tokenization complete for train and validation sets.\")\n",
    "\n",
    "    output_dir = \"./fine_tuned_qa_model_with_eval\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs_with_eval',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # threshold: minimum change to qualify as an improvement\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=3, # Wait for 3 epochs without improvement\n",
    "        early_stopping_threshold=0.01 # A minimal improvement threshold\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    print(\"Trainer set up with evaluation and early stopping.\")\n",
    "    trainer.train()\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(\"Fine-tuned model and tokenizer saved.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing required libraries: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4GZJGMFxUNf"
   },
   "outputs": [],
   "source": [
    "# Using Fine Tuned Model\n",
    "\n",
    "fine_tuned_model_path = output_dir\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained(fine_tuned_model_path).to(device)\n",
    "qa_pipeline_fine_tuned = pipeline(\"question-answering\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "test_question_2 = \"What is AI?\"\n",
    "test_context_2 = dummy_context_2\n",
    "answer_ft_2 = qa_pipeline_fine_tuned(question=test_question_2, context=test_context_2)\n",
    "print(f\"\\nQuestion: {test_question_2}\")\n",
    "print(f\"Fine-tuned Answer: {answer_ft_2['answer']} (Score: {answer_ft_2['score']:.4f})\")\n",
    "\n",
    "    # Pick an example that ended up in the validation set\n",
    "if len(eval_dataset) > 0:\n",
    "    val_example = full_dataset[val_indices[0]]\n",
    "    val_question = val_example[\"question\"]\n",
    "    val_context = val_example[\"context\"]\n",
    "    val_answer = qa_pipeline_fine_tuned(question=val_question, context=val_context)\n",
    "    print(f\"\\nValidation Question: {val_question}\")\n",
    "    print(f\"Fine-tuned Answer: {val_answer['answer']} (Score: {val_answer['score']:.4f})\")\n",
    "else:\n",
    "    print(\"No validation examples to test directly.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODwxL8ZrV1Lz"
   },
   "outputs": [],
   "source": [
    "# Confidence Score Calculation and Interpretation\n",
    "if 'qa_pipeline_fine_tuned' in globals() and qa_pipeline_fine_tuned is not None:\n",
    "    current_qa_pipeline = qa_pipeline_fine_tuned\n",
    "elif 'qa_pipeline' in globals() and qa_pipeline is not None:\n",
    "    current_qa_pipeline = qa_pipeline\n",
    "else:\n",
    "    print(\"Error: QA pipeline not initialized.\")\n",
    "    current_qa_pipeline = None # Set to None to prevent further errors\n",
    "\n",
    "if current_qa_pipeline:\n",
    "    print(\"\\n--- Integrating Confidence Threshold into Multi-Document QA ---\")\n",
    "    def get_top_reliable_answer_across_docs(question, documents_dict, qa_pipeline_func, overall_threshold=0.6):\n",
    "        all_ranked_answers = answer_question_with_chunking(question, documents_dict)\n",
    "\n",
    "        if not all_ranked_answers:\n",
    "            return \"No answer found.\", 0.0, None\n",
    "\n",
    "        top_answer = all_ranked_answers[0]\n",
    "        if top_answer['score'] >= overall_threshold:\n",
    "            print(f\"  Top Reliable Answer (from {top_answer['source_document']}):\")\n",
    "            print(f\"    Answer: '{top_answer['answer']}'\")\n",
    "            print(f\"    Confidence: {top_answer['score']:.4f}\")\n",
    "            print(f\"    Context Preview: '{top_answer['context_preview']}'\")\n",
    "            return top_answer['answer'], top_answer['score'], top_answer['source_document']\n",
    "        else:\n",
    "            return \"No reliable answer found across documents.\", 0.0, None\n",
    "\n",
    "    get_top_reliable_answer_across_docs(\"What does BERT stand for?\", all_documents_text, current_qa_pipeline, overall_threshold=0.8)\n",
    "    get_top_reliable_answer_across_docs(\"What is the capital of France?\", all_documents_text, current_qa_pipeline, overall_threshold=0.7)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Confidence Score Calculation as QA pipeline is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj2UxIr4-qmq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lowercases, removes punctuation and articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_tokens = get_tokens(a_gold)\n",
    "    pred_tokens = get_tokens(a_pred)\n",
    "\n",
    "    common = Counter(gold_tokens) & Counter(pred_tokens)\n",
    "    num_common = sum(common.values())\n",
    "\n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(gold_tokens)\n",
    "\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "def compute_exact_match(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def evaluate_qa_predictions(predictions, gold_standard_data):\n",
    "    results = {}\n",
    "    gold_map = {item['question']: item['reference_answers'] for item in gold_standard_data}\n",
    "\n",
    "    for pred in predictions:\n",
    "        question = pred['question']\n",
    "        predicted_answer = pred['predicted_answer']\n",
    "        method = pred['method'] # e.g., 'tfidf', 'bow', 'word_embedding', 'bert'\n",
    "\n",
    "        if method not in results:\n",
    "            results[method] = {'em_scores': [], 'f1_scores': []}\n",
    "\n",
    "        reference_answers = gold_map.get(question, [])\n",
    "        if not reference_answers:\n",
    "            print(f\"Warning: No reference answers found for question: '{question}'. Skipping evaluation for this prediction.\")\n",
    "            continue\n",
    "\n",
    "        em_for_question = 0\n",
    "        f1_for_question = 0\n",
    "\n",
    "        # For each prediction, compare against all possible reference answers\n",
    "        # and take the max EM/F1\n",
    "        for gold_answer in reference_answers:\n",
    "            em_for_question = max(em_for_question, compute_exact_match(gold_answer, predicted_answer))\n",
    "            f1_for_question = max(f1_for_question, compute_f1(gold_answer, predicted_answer))\n",
    "\n",
    "        results[method]['em_scores'].append(em_for_question)\n",
    "        results[method]['f1_scores'].append(f1_for_question)\n",
    "\n",
    "    final_scores = {}\n",
    "    for method, scores in results.items():\n",
    "        avg_em = np.mean(scores['em_scores']) if scores['em_scores'] else 0\n",
    "        avg_f1 = np.mean(scores['f1_scores']) if scores['f1_scores'] else 0\n",
    "        final_scores[method] = {'average_em': avg_em, 'average_f1': avg_f1}\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a Gold Standard Dataset\n",
    "    # Note: 'context' is for QA models; for evaluation, we only need question and reference_answers\n",
    "    gold_data = [\n",
    "        {'question': 'What is BERT?',\n",
    "         'context': 'BERT (Bidirectional Encoder Representations from Transformers) is a language model developed by Google.',\n",
    "         'reference_answers': ['BERT is a language model developed by Google', 'a language model developed by Google']},\n",
    "\n",
    "        {'question': 'The capital of France?',\n",
    "         'context': 'The capital of France is Paris. Paris is also known for its beautiful architecture like the Eiffel Tower.',\n",
    "         'reference_answers': ['Paris', 'The capital of France is Paris']},\n",
    "\n",
    "        {'question': 'How does quantum computing work?',\n",
    "         'context': 'Quantum computing uses phenomena such as superposition and entanglement to perform computations.',\n",
    "         'reference_answers': ['uses phenomena such as superposition and entanglement to perform computations', 'superposition and entanglement']},\n",
    "\n",
    "        {'question': 'What is the best type of animal?',\n",
    "         'context': 'Dogs are loyal. Cats are independent.',\n",
    "         'reference_answers': ['']}\n",
    "    ]\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for item in gold_data:\n",
    "        question = item['question']\n",
    "\n",
    "        # TF-IDF prediction\n",
    "        tfidf_result = get_answer_classical_nlp(question, all_documents_text, method=\"tfidf\")\n",
    "        all_predictions.append({\n",
    "            'question': question,\n",
    "            'predicted_answer': tfidf_result['answer'],\n",
    "            'method': 'TF-IDF',\n",
    "            'confidence': tfidf_result['confidence'],\n",
    "            'source_document': tfidf_result['source_document']\n",
    "        })\n",
    "\n",
    "        # BoW prediction\n",
    "        bow_result = get_answer_classical_nlp(question, all_documents_text, method=\"bow\")\n",
    "        all_predictions.append({\n",
    "            'question': question,\n",
    "            'predicted_answer': bow_result['answer'],\n",
    "            'method': 'BoW',\n",
    "            'confidence': bow_result['confidence'],\n",
    "            'source_document': bow_result['source_document']\n",
    "        })\n",
    "\n",
    "        # Word Embedding prediction (GloVe 50)\n",
    "        we_result = get_answer_classical_nlp(question, all_documents_text, method=\"word_embedding\", word_embedding_model_name=\"glove-wiki-gigaword-50\")\n",
    "        all_predictions.append({\n",
    "            'question': question,\n",
    "            'predicted_answer': we_result['answer'],\n",
    "            'method': 'Word_Embedding',\n",
    "            'confidence': we_result['confidence'],\n",
    "            'source_document': we_result['source_document']\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    # Simulate BERT predictions\n",
    "    # For a real BERT model, you would feed (question, context) pairs to it.\n",
    "    # The 'context' would be the relevant document from your `loaded_documents_for_qa`.\n",
    "    bert_simulated_predictions = [\n",
    "        {'question': 'What is BERT?', 'predicted_answer': 'a language model developed by Google', 'method': 'BERT', 'confidence': 0.98, 'source_document': 'doc_qa_1'},\n",
    "        {'question': 'The capital of France?', 'predicted_answer': 'Paris', 'method': 'BERT', 'confidence': 0.99, 'source_document': 'doc_qa_2'},\n",
    "        {'question': 'How does quantum computing work?', 'predicted_answer': 'uses phenomena such as superposition and entanglement to perform computations', 'method': 'BERT', 'confidence': 0.97, 'source_document': 'doc_qa_3'},\n",
    "        {'question': 'What is the best type of animal?', 'predicted_answer': 'Dogs are loyal.', 'method': 'BERT', 'confidence': 0.05, 'source_document': 'doc_qa_4'} # Low confidence for unanswerable\n",
    "    ]\n",
    "    all_predictions.extend(bert_simulated_predictions)\n",
    "\n",
    "\n",
    "    # Evaluate all predictions\n",
    "    evaluation_scores = evaluate_qa_predictions(all_predictions, gold_data)\n",
    "\n",
    "    # 3. Print Results\n",
    "    for method, scores in evaluation_scores.items():\n",
    "        print(f\"\\nMethod: {method}\")\n",
    "        print(f\"  Average Exact Match (EM): {scores['average_em']:.4f}\")\n",
    "        print(f\"  Average F1-score: {scores['average_f1']:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n--- Detailed Predictions vs. Gold Standard ---\")\n",
    "    for item in gold_data:\n",
    "        question = item['question']\n",
    "        reference_answers = item['reference_answers']\n",
    "        print(f\"\\nQuestion: '{question}'\")\n",
    "        print(f\"  Reference Answers: {reference_answers}\")\n",
    "\n",
    "        for pred in all_predictions:\n",
    "            if pred['question'] == question:\n",
    "                em = max([compute_exact_match(gold_ans, pred['predicted_answer']) for gold_ans in reference_answers])\n",
    "                f1 = max([compute_f1(gold_ans, pred['predicted_answer']) for gold_ans in reference_answers])\n",
    "                print(f\"    - {pred['method']}: Predicted: '{pred['predicted_answer']}' (EM: {em:.2f}, F1: {f1:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgS2u5t4-qcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7BgZ6I41XIKbFP9cm5Gi5",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
